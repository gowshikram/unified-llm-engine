# âš¡ Unified LLM Engine

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**Multi-provider LLM abstraction layer** with unified API, automatic fallbacks, cost tracking, and intelligent routing across OpenAI, Anthropic, Google, and more.

---

## ğŸŒŸ Features

- **Unified API** - Same interface for all providers
- **6+ Providers** - OpenAI, Anthropic, Gemini, Azure, AWS Bedrock, Ollama
- **Automatic Fallbacks** - Seamless failover between providers
- **Cost Tracking** - Token usage and cost monitoring
- **Response Caching** - Reduce costs with intelligent caching
- **Streaming Support** - Real-time response streaming
- **Async-First** - Built for high-performance async execution

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Unified LLM Engine                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚                   LLM Router                              â”‚    â”‚
â”‚  â”‚  â€¢ Model Selection  â€¢ Load Balancing  â€¢ Fallback Logic   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  OpenAI    â”‚  â”‚ Anthropic  â”‚  â”‚  Gemini    â”‚  â”‚  Ollama  â”‚  â”‚
â”‚  â”‚  Provider  â”‚  â”‚  Provider  â”‚  â”‚  Provider  â”‚  â”‚ Provider â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                              â†“                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Response Cache + Cost Tracker               â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/yourusername/unified-llm-engine.git
cd unified-llm-engine
pip install -r requirements.txt
```

### Basic Usage

```python
from llm_engine import LLMEngine

# Initialize with multiple providers
engine = LLMEngine()

# Use like any single LLM
response = await engine.generate(
    prompt="Explain quantum computing in simple terms",
    model="gpt-4",  # or "claude-3-opus", "gemini-pro", etc.
    temperature=0.7,
    max_tokens=500
)

print(response.content)
print(response.usage)  # Token counts
print(response.cost)   # Estimated cost
```

### Provider-Specific

```python
from llm_engine.providers import OpenAIProvider, AnthropicProvider

# Use specific provider
openai = OpenAIProvider(api_key="sk-...")
response = await openai.generate(prompt="Hello!", model="gpt-4")

# With fallback
anthropic = AnthropicProvider(api_key="sk-ant-...")
response = await engine.generate(
    prompt="Hello!",
    model="gpt-4",
    fallback_models=["claude-3-opus", "gemini-pro"]
)
```

---

## ğŸ“š Providers

| Provider | Models | Streaming | Embeddings |
|----------|--------|-----------|------------|
| **OpenAI** | GPT-4, GPT-3.5 | âœ… | âœ… |
| **Anthropic** | Claude 3 Opus/Sonnet/Haiku | âœ… | âŒ |
| **Gemini** | Gemini Pro, Gemini Flash | âœ… | âœ… |
| **Azure OpenAI** | GPT-4, GPT-3.5 | âœ… | âœ… |
| **AWS Bedrock** | Claude, Titan | âœ… | âœ… |
| **Ollama** | Llama, Mistral, etc. | âœ… | âœ… |

---

## ğŸ“ Project Structure

```
unified-llm-engine/
â”œâ”€â”€ llm_engine/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â””â”€â”€ engine.py
â”‚   â””â”€â”€ providers/
â”‚       â”œâ”€â”€ base_provider.py
â”‚       â”œâ”€â”€ openai_provider.py
â”‚       â”œâ”€â”€ anthropic_provider.py
â”‚       â”œâ”€â”€ gemini_provider.py
â”‚       â””â”€â”€ exceptions.py
â”œâ”€â”€ examples/
â”œâ”€â”€ tests/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

```bash
# .env
OPENAI_API_KEY=sk-...
ANTHROPIC_API_KEY=sk-ant-...
GEMINI_API_KEY=...

# Optional
DEFAULT_MODEL=gpt-4
ENABLE_CACHING=true
CACHE_TTL=3600
```

---

## ğŸ“Š Cost Tracking

```python
from llm_engine import LLMEngine

engine = LLMEngine(track_costs=True)

# Make requests...
response = await engine.generate(...)

# Get cost summary
print(engine.get_cost_summary())
# {
#   "total_cost": 0.054,
#   "total_tokens": 1250,
#   "by_model": {"gpt-4": 0.050, "claude-3-sonnet": 0.004}
# }
```

---

## ğŸ“„ License

MIT License - See [LICENSE](LICENSE)

---

## ğŸ“¬ Contact

**Ravi Teja K** - AI/ML Engineer
- GitHub: [@TEJA4704](https://github.com/TEJA4704)
